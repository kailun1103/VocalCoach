import os
from pathlib import Path

from pydantic import Field
from pydantic_settings import BaseSettings

_APP_DIR = Path(__file__).resolve().parent
_MODEL_DIR = _APP_DIR / "model"
_STT_DIR = _MODEL_DIR / "stt"
_TTS_DIR = _MODEL_DIR / "tts"
_TTS_RUNTIME_DIR = _TTS_DIR / "runtime"
_TTS_VOICES_DIR = _TTS_DIR / "voices"


def _default_whisper_binary() -> Path:
    """
    Locate a whisper.cpp executable, preferring a local build in backend/app/model.
    Returns the most likely candidate so deployments can override via env vars if needed.
    """
    for candidate in ("whisper-cli.exe", "main.exe", "whisper-cli", "main"):
        binary_path = _STT_DIR / candidate
        if binary_path.exists():
            return binary_path
    return _STT_DIR / "whisper-cli.exe"


def _default_whisper_model() -> Path:
    """Point to the bundled ggml-small.en-q5_1.bin model by default."""
    return _STT_DIR / "ggml-small.en-q5_1.bin"


class Settings(BaseSettings):
    """Application configuration loaded from environment variables or defaults."""

    whisper_binary: Path = Field(
        default_factory=_default_whisper_binary,
        description="Path to whisper.cpp executable.",
    )
    whisper_model: Path = Field(
        default_factory=_default_whisper_model,
        description="Path to whisper.cpp model file.",
    )
    whisper_threads: int = Field(
        default_factory=lambda: os.cpu_count() or 1,
        description="Number of CPU threads allocated to whisper.cpp.",
    )
    whisper_beam_size: int = Field(
        default=1,
        description="Beam size used during decoding (higher improves quality but is slower).",
    )
    whisper_best_of: int = Field(
        default=1,
        description="Number of best candidates to keep (higher improves quality but is slower).",
    )
    whisper_temperature: float = Field(
        default=0.0,
        description="Sampling temperature for whisper.cpp decoding.",
    )
    whisper_print_timestamps: bool = Field(
        default=False,
        description="Whether to include timestamps in transcription output.",
    )
    piper_binary: Path = Field(
        default=_TTS_RUNTIME_DIR / "piper.exe",
        description="Path to Piper executable.",
    )
    piper_model: Path = Field(
        default=_TTS_VOICES_DIR / "en_US-amy-medium.onnx",
        description="Path to Piper voice model.",
    )
    data_directory: Path = Field(
        default=Path("./data"),
        description="Base directory for persistent audio artifacts generated by the API.",
    )
    use_mock_services: bool = Field(
        default=False,
        description="Toggle to fall back to mock implementations when models are unavailable.",
    )
    mock_language: str = Field(
        default="en",
        description="Default language code used for mock data generation.",
    )

    # LLM (OpenAI-compatible) endpoint settings
    llm_base_url: str = Field(
        default="http://127.0.0.1:1234/v1",
        description="Base URL of local OpenAI-compatible LLM server.",
    )
    llm_default_model: str | None = Field(
        default=None,
        description="Default chat model name; request may override.",
    )

    class Config:
        env_file = ".env"
        env_file_encoding = "utf-8"


settings = Settings()
